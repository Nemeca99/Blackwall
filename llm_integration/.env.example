# Example .env file for LLM Integration
# Place this file in the BlackwallV2/Implementation directory

# Main LLM provider: local, openai, anthropic, litellm
LLM_PROVIDER=local

# API key for the selected provider
# LLM_API_KEY=sk-your-api-key-here

# Model to use
LLM_MODEL=gpt-4

# Generation parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1024
LLM_TOP_P=0.9
LLM_FREQUENCY_PENALTY=0.0
LLM_PRESENCE_PENALTY=0.0

# Local API URL for local LLM servers (e.g., LM Studio, Ollama)
LLM_LOCAL_API_URL=http://localhost:1234/v1/chat/completions
